{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfw/dD3BJb24iWFtIcBPXU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ARUNRAJDAARA/ARUNRAJDAARA/blob/main/NLP_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYeu673__j4I"
      },
      "outputs": [],
      "source": [
        "data = \"Tokenization is the process of splitting text into smaller units, called tokens.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lower_data=data.lower()"
      ],
      "metadata": {
        "id": "sidZlCs6_zYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenizing using NLTK\n",
        "tokens = word_tokenize(lower_data)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH3XJiUt__MC",
        "outputId": "d24663ea-67a3-4d9c-86e9-fd8ecc7d253c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tokenization', 'is', 'the', 'process', 'of', 'splitting', 'text', 'into', 'smaller', 'units', ',', 'called', 'tokens', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8El8GSXAlsn",
        "outputId": "e6d8cdd0-c722-49b4-ec29-fe1ee9a2f134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tokenization',\n",
              " 'is',\n",
              " 'the',\n",
              " 'process',\n",
              " 'of',\n",
              " 'splitting',\n",
              " 'text',\n",
              " 'into',\n",
              " 'smaller',\n",
              " 'units',\n",
              " ',',\n",
              " 'called',\n",
              " 'tokens',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**printing the all stopwords in the english:**"
      ],
      "metadata": {
        "id": "_UXZL6wELIrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Get the list of English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Print all stop words\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gCF_aXhAQUB",
        "outputId": "10552187-5ee1-40c6-f899-dbc79aaff610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'d', 'they', \"don't\", 'm', 'were', 'been', 'but', 'or', 'whom', 'theirs', 'your', 'are', 'when', \"couldn't\", \"haven't\", 'for', 'shouldn', 'those', 'off', \"needn't\", 'was', 'weren', 'ain', 'more', 'didn', 'doing', \"hadn't\", 'an', 'hasn', 'until', 'am', 'below', 'because', 'under', 'once', 'no', 'his', 'into', 'there', 'isn', 'myself', 'mightn', 't', \"won't\", 'any', 'you', 'between', 'i', 'very', 've', 'do', 'can', 'yourself', 'both', 'about', 'so', 'being', 'he', 'mustn', 'did', 'how', 'we', 'again', 'too', \"wouldn't\", 'having', 'herself', 'this', \"you'll\", 'haven', 'same', \"shan't\", 're', 'itself', 'as', 'ourselves', 'above', 'from', 'most', \"that'll\", 'only', \"you'd\", 'himself', 'then', 'had', 'ma', 'than', \"hasn't\", 'the', 'and', 'a', \"shouldn't\", 'of', 'not', 'wasn', 'which', \"isn't\", 'o', 'she', 'hers', 'shan', 'will', 'our', 'her', 'in', 'should', \"mustn't\", 'against', \"should've\", 'with', 'here', \"wasn't\", 'ours', 'is', 'by', 'couldn', 'does', \"mightn't\", \"you've\", 'needn', 'be', 'through', 'where', 'each', 'just', 'other', 'now', 'to', 'before', 'own', 'it', 'on', 'y', 'aren', \"didn't\", 'down', 'll', 'while', 'that', 'after', 'up', 'such', 'these', \"weren't\", 'hadn', 'won', 'why', 'who', 'me', 'some', 'yourselves', 'out', 'nor', \"she's\", 'my', 'over', \"you're\", 'their', 'what', 'yours', 'has', 'themselves', 'have', 'them', 'doesn', \"doesn't\", \"aren't\", 'all', 's', 'at', 'if', 'further', 'wouldn', 'during', 'him', 'its', 'don', 'few', \"it's\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applaying stop words :**"
      ],
      "metadata": {
        "id": "KbdFgRDnLCzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LLIBcvJAsbX",
        "outputId": "ca8a2f5d-33ab-4ffa-b7f7-0a02089778ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tokenization', 'process', 'splitting', 'text', 'smaller', 'units', ',', 'called', 'tokens', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Printing the stop words present in our Data :**"
      ],
      "metadata": {
        "id": "PFQmdd7kK5Ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Sample text\n",
        "data = \"Tokenization is the process of splitting text into smaller units, called tokens.\"\n",
        "lower_data = data.lower()\n",
        "\n",
        "# Tokenizing using NLTK\n",
        "tokens = word_tokenize(lower_data)\n",
        "\n",
        "# Load the list of stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Find stop words present in the data\n",
        "stop_words_in_data = [word for word in tokens if word in stop_words]\n",
        "\n",
        "# Remove duplicates (if any)\n",
        "stop_words_in_data = list(set(stop_words_in_data))\n",
        "print(\"This is my orginal data : \",data)\n",
        "print()\n",
        "print(\"these are Stop Words present in my  data:\", stop_words_in_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVp8w-CqCz8l",
        "outputId": "dd188f7c-dc55-46c3-af96-ae4bcb32800a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is my orginal data :  Tokenization is the process of splitting text into smaller units, called tokens.\n",
            "\n",
            "these are Stop Words present in my  data: ['is', 'of', 'the', 'into']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xe4EJqaGL1G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kgigz3kPDdpx",
        "outputId": "a9b66a69-1dcb-425f-c259-f2ec5150ca54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tokenization',\n",
              " 'process',\n",
              " 'splitting',\n",
              " 'text',\n",
              " 'smaller',\n",
              " 'units',\n",
              " ',',\n",
              " 'called',\n",
              " 'tokens',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in filtered_tokens:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTOdRgYMEDf8",
        "outputId": "97c09e66-fef4-47d8-c246-030a0cdce1ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenization\n",
            "process\n",
            "splitting\n",
            "text\n",
            "smaller\n",
            "units\n",
            ",\n",
            "called\n",
            "tokens\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming :**"
      ],
      "metadata": {
        "id": "47BGr2GdKyiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer # Import the PorterStemmer\n",
        "from nltk.tokenize import word_tokenize # Make sure word_tokenize is imported\n",
        "\n",
        "stemmer = PorterStemmer() # Create a stemmer object\n",
        "\n",
        "for token in filtered_tokens: # Iterate over each token in the list\n",
        "    stemmed_token = stemmer.stem(token) # Stem the token\n",
        "    print(stemmed_token) # Print the stemmed token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVAHw37OExS8",
        "outputId": "d100016b-7533-4273-91ce-736b1da97ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token\n",
            "process\n",
            "split\n",
            "text\n",
            "smaller\n",
            "unit\n",
            ",\n",
            "call\n",
            "token\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization :**"
      ],
      "metadata": {
        "id": "WAdkhNYmKtbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()  # Create a lemmatizer object\n",
        "\n",
        "for token in filtered_tokens:  # Iterate over each token in the list\n",
        "    lemmatized_token = lemmatizer.lemmatize(token)  # Lemmatize the token\n",
        "    print(lemmatized_token)  # Print the lemmatized token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KxHQSryGMKX",
        "outputId": "36d4dd98-1f69-4e72-e365-15c54d1b3d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenization\n",
            "process\n",
            "splitting\n",
            "text\n",
            "smaller\n",
            "unit\n",
            ",\n",
            "called\n",
            "token\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag of words :**"
      ],
      "metadata": {
        "id": "BYNvRNsBKP_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text  import CountVectorizer\n",
        "cv=CountVectorizer()"
      ],
      "metadata": {
        "id": "zu9RFnjJG0UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=cv.fit_transform( filtered_tokens)"
      ],
      "metadata": {
        "id": "hrSEZMeaHCA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FZ0Un63iIWO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1NsxyI9HsI3",
        "outputId": "43fb651b-2945-4638-8098-42a405080127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tokenization': 5,\n",
              " 'process': 1,\n",
              " 'splitting': 3,\n",
              " 'text': 4,\n",
              " 'smaller': 2,\n",
              " 'units': 7,\n",
              " 'called': 0,\n",
              " 'tokens': 6}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PinjUkUH_bO",
        "outputId": "9fb96220-4821-46ce-ef4a-128c642fafe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tokenization',\n",
              " 'process',\n",
              " 'splitting',\n",
              " 'text',\n",
              " 'smaller',\n",
              " 'units',\n",
              " ',',\n",
              " 'called',\n",
              " 'tokens',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7S-OIu6RIKFS",
        "outputId": "a6720039-6ce3-41f9-f593-cbfac145cb5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tokenization'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0].toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxgR55UIIXNJ",
        "outputId": "040830d0-af00-4855-896f-e408a5e22c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 1, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[1].toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocP7NHsTIewe",
        "outputId": "f5a7f30b-d47c-4660-ae32-d49b777b345c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[9].toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4azVVTbIiwh",
        "outputId": "193ba894-9ca3-4b60-e326-a6418ecc0805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0].toarray().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdwfCQOjIyTw",
        "outputId": "3c5754e1-1573-4d1e-e832-9f66dc51cfc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Binar Bag of words :**"
      ],
      "metadata": {
        "id": "4TSjTl2nKa6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text  import CountVectorizer\n",
        "cv=CountVectorizer(binary=True)\n"
      ],
      "metadata": {
        "id": "G76w1rgII6H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=cv.fit_transform( filtered_tokens)"
      ],
      "metadata": {
        "id": "-0WZbETvJjuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SQrKBg8Js1X",
        "outputId": "b4ff6627-dfe0-41b1-85f0-2db9c7082fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tokenization': 5,\n",
              " 'process': 1,\n",
              " 'splitting': 3,\n",
              " 'text': 4,\n",
              " 'smaller': 2,\n",
              " 'units': 7,\n",
              " 'called': 0,\n",
              " 'tokens': 6}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0].toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFn63ALrJ9h9",
        "outputId": "4bd40107-44d8-4680-d9d6-fde7e3d86588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 1, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "89qneHTsKFX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U12MQKKQL2UZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EKgmGVNUL2fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Sks79s-L2hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4yQXNjtIL2lI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}